{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping \n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate  noice data\n",
    "def noice_data(noice_factor):\n",
    "    \n",
    "    \n",
    "    X_train_noisy=X_train + noice_factor*np.random.normal(loc=0.0, scale = 1.0, size=X_train.shape)\n",
    "    X_test_noisy=X_test + noice_factor*np.random.normal(loc=0.0, scale = 1.0, size=X_test.shape)\n",
    "    \n",
    "    X_train_noisy = np.clip(X_train_noisy,0.,1.)\n",
    "    X_test_noisy = np.clip(X_test_noisy,0.,1.) \n",
    "    return X_train_noisy,X_test_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 25s 460us/step - loss: 1.3918 - accuracy: 0.5989 - val_loss: 0.5194 - val_accuracy: 0.8598\n",
      "Noice factor: 0.00%, Accuracy: 83.79%\n",
      "0.05\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 20s 367us/step - loss: 2.3013 - accuracy: 0.1126 - val_loss: 2.3010 - val_accuracy: 0.1050\n",
      "Noice factor: 0.05%, Accuracy: 11.35%\n",
      "0.1\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "  800/54000 [..............................] - ETA: 11:07 - loss: 2.3003 - accuracy: 0.1238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.181634). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 28s 517us/step - loss: 2.3018 - accuracy: 0.1123 - val_loss: 2.3018 - val_accuracy: 0.1050\n",
      "Noice factor: 0.10%, Accuracy: 11.35%\n",
      "0.15000000000000002\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "54000/54000 [==============================] - 27s 499us/step - loss: 2.3020 - accuracy: 0.1114 - val_loss: 2.3028 - val_accuracy: 0.1050\n",
      "Noice factor: 0.15%, Accuracy: 11.35%\n",
      "0.2\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "  800/54000 [..............................] - ETA: 31:26 - loss: 2.3123 - accuracy: 0.1200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.258416). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 47s 875us/step - loss: 2.3021 - accuracy: 0.1121 - val_loss: 2.3020 - val_accuracy: 0.1050\n",
      "Noice factor: 0.20%, Accuracy: 11.35%\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "for noice_fact in np.arange(0,1.050,0.050):\n",
    "    print(noice_fact)\n",
    "    X_train_noisy,X_test_noisy = noice_data(noice_fact)\n",
    "    \n",
    "    model = create_model()    \n",
    "    model.fit(X_train_noisy, y_train, validation_split=0.1, epochs=1, batch_size=200,callbacks=[callback])\n",
    "    \n",
    "    scores = model.evaluate(X_test_noisy, y_test, verbose=0)\n",
    "    print(\"Noice factor: %.2f%%, Accuracy: %.2f%%\" % (noice_fact,scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
